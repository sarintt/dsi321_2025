# DSI321

# Twitter Scraping & Analysis Pipeline
A modular data pipeline that automates scraping tweets from Twitter (X), validates and stores data in LakeFS, orchestrates scheduled flows with Prefect, and visualizes insights through an interactive Streamlit dashboard. Built for flexibility, automation, and real-world data analysis.

# Project Status
| Module / Tool | Status |
| - | :-: |
| Modern Logging (Logging, Rich) | Check |
| Web Scraping | Check |
| Database(LakeFS) | Check |
| Data Validation (Pydantic) | Check |
| Orchestration (Prefect) Part 1: All tweets| Check |
| Orchestration (Prefect) Part 2: Only new tweets| Check |
| ML (Word Cloud)| Check |
| Web Interface (Streamlit) | Check |

# ðŸ“Š Data Schema

This project enforces a strict schema and data validation protocol to ensure data consistency and integrity.  
Below is the expected schema of the processed dataset (`data.parquet`):

| Column       | Data Type        |
|--------------|------------------|
| category     | string[python]   |
| tag          | string[python]   |
| username     | string[python]   |
| tweetText    | string[python]   |
| postTimeRaw  | datetime64[ns]   |
| scrapeTime   | datetime64[ns]   |
| year         | int64            |
| month        | int64            |
| day          | int64            |

## Schema Validation

| Check                             | Result   |
|----------------------------------|----------|
| Schema matches original format   | True  |
| Number of records > 1000         | True  |
| Check data duplicate records                | 0     |
| Null values in all columns       | 0     |
| Data types are consistent        | True  |

# Project Structure
```
.
â”œâ”€â”€ config                          # Configuration files for Docker, logging, and paths
â”‚   â”œâ”€â”€ docker                        
â”‚   â”‚   â”œâ”€â”€ Dockerfile.cli          # Dockerfile for CLI usage
â”‚   â”‚   â””â”€â”€ Dockerfile.worker       # Dockerfile for worker services
â”‚   â”œâ”€â”€ logging
â”‚   â”‚   â””â”€â”€ modern_log.py           # Custom logging configuration
â”‚   â””â”€â”€ path_config.py              # Path configuration for file management
â”œâ”€â”€ src                             # Source code directory
â”‚   â”œâ”€â”€ backend                     # Backend logic for scraping, validation, loading
â”‚   â”‚   â”œâ”€â”€ load
â”‚   â”‚   â”‚   â””â”€â”€ lakefs_loader.py    # Module for loading data to lakeFS
â”‚   â”‚   â”œâ”€â”€ pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ incremental_scrape_flow.py   # Scraping flow for incremental data
â”‚   â”‚   â”‚   â””â”€â”€ initial_scrape_flow.py       # Scraping flow for initial/full data
â”‚   â”‚   â”œâ”€â”€ scraping
â”‚   â”‚   â”‚   â”œâ”€â”€ x_login.py          # Script to log in to X 
â”‚   â”‚   â”‚   â””â”€â”€ x_scraping.py       # Script to scrape data from X
â”‚   â”‚   â””â”€â”€ validation
â”‚   â”‚   â”‚   â””â”€â”€ validate.py         # Data validation logic
â”‚   â””â”€â”€ fronend                     # Frontend components (Note: typo, should be "frontend")
â”‚       â””â”€â”€ streamlit.py            # Streamlit app for data display
â”œâ”€â”€ test                            # Unit and integration test files
â”œâ”€â”€ .env.example                    # Example of environment variable file
â”œâ”€â”€ .gitignore                      # Git ignore rules
â”œâ”€â”€ README.md                       # Project documentation
â”œâ”€â”€ docker-compose.yml              # Docker Compose configuration
â”œâ”€â”€ pyproject.toml                  # Python project configuration
â”œâ”€â”€ requirements.txt                # Python package requirements
â””â”€â”€ start.sh                        # Startup script for the project
```

# Prepare
1. Create a virtual environment
```bash
python -m venv .venv
```
2. Activate the virtual environment
    - Windows
        ```bash
        source .venv/Scripts/activate
        ```
    - macOS & Linux
        ```bash
        source .venv/bin/activate
        ```
3. Run the startup script
```bash
bash start.sh
# or
./start.sh
```

# Running Prefect
1. Start the Prefect server
```bash
docker compose --profile server up -d
```
2. Connect to the CLI container
```bash
docker compose run cli
```
3. Run the initial scraping flow (to collect all tweets for base data)
```bash
python src/backend/pipeline/initial_scrape_flow.py
```
4. Schedule scraping every 15 minutes (incremental updates)
```bash
python src/backend/pipeline/incremental_scrape_flow.py
```
- **View the Prefect flow UI**
Open your browser and go to: http://localhost:42000 
