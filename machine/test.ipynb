{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "tweets-repo/main/tweets.parquet/year=2019/month=1/day=30/1585f9c156584d668f3fb6cbfbc5653e-0.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33mtweets.parquet/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m lakefs_s3_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlakefs_s3_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m df.sort_values(by=[\u001b[33m'\u001b[39m\u001b[33mpostTimeRaw\u001b[39m\u001b[33m'\u001b[39m], ascending=\u001b[38;5;28;01mTrue\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     24\u001b[39m df.drop_duplicates(subset=\u001b[33m\"\u001b[39m\u001b[33mtweetText\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dsi321_2025/.venv/lib/python3.13/site-packages/pandas/io/parquet.py:667\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    665\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dsi321_2025/.venv/lib/python3.13/site-packages/pandas/io/parquet.py:274\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    268\u001b[39m     path,\n\u001b[32m    269\u001b[39m     filesystem,\n\u001b[32m    270\u001b[39m     storage_options=storage_options,\n\u001b[32m    271\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    272\u001b[39m )\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     result = pa_table.to_pandas(**to_pandas_kwargs)\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dsi321_2025/.venv/lib/python3.13/site-packages/pyarrow/parquet/core.py:1824\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[32m   1813\u001b[39m     dataset = ParquetFile(\n\u001b[32m   1814\u001b[39m         source, read_dictionary=read_dictionary,\n\u001b[32m   1815\u001b[39m         memory_map=memory_map, buffer_size=buffer_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1821\u001b[39m         page_checksum_verification=page_checksum_verification,\n\u001b[32m   1822\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1824\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1825\u001b[39m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dsi321_2025/.venv/lib/python3.13/site-packages/pyarrow/parquet/core.py:1475\u001b[39m, in \u001b[36mParquetDataset.read\u001b[39m\u001b[34m(self, columns, use_threads, use_pandas_metadata)\u001b[39m\n\u001b[32m   1467\u001b[39m         index_columns = [\n\u001b[32m   1468\u001b[39m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[32m   1469\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   1470\u001b[39m         ]\n\u001b[32m   1471\u001b[39m         columns = (\n\u001b[32m   1472\u001b[39m             \u001b[38;5;28mlist\u001b[39m(columns) + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) - \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[32m   1473\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1475\u001b[39m table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1477\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1480\u001b[39m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[32m   1481\u001b[39m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dsi321_2025/.venv/lib/python3.13/site-packages/pyarrow/_dataset.pyx:589\u001b[39m, in \u001b[36mpyarrow._dataset.Dataset.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dsi321_2025/.venv/lib/python3.13/site-packages/pyarrow/_dataset.pyx:3941\u001b[39m, in \u001b[36mpyarrow._dataset.Scanner.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dsi321_2025/.venv/lib/python3.13/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dsi321_2025/.venv/lib/python3.13/site-packages/pyarrow/error.pxi:89\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dsi321_2025/.venv/lib/python3.13/site-packages/pyarrow/_fs.pyx:1557\u001b[39m, in \u001b[36mpyarrow._fs._cb_open_input_file\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dsi321_2025/.venv/lib/python3.13/site-packages/pyarrow/fs.py:419\u001b[39m, in \u001b[36mFSSpecHandler.open_input_file\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PythonFile\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fs.isfile(path):\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PythonFile(\u001b[38;5;28mself\u001b[39m.fs.open(path, mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m), mode=\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: tweets-repo/main/tweets.parquet/year=2019/month=1/day=30/1585f9c156584d668f3fb6cbfbc5653e-0.parquet"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "lakefs_endpoint = \"http://localhost:8001/\"\n",
    "storage_options = {\n",
    "    \"key\": os.getenv(\"ACCESS_KEY\"),\n",
    "    \"secret\": os.getenv(\"SECRET_KEY\"),\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": lakefs_endpoint\n",
    "    }\n",
    "}\n",
    "repo_name = \"tweets-repo\"\n",
    "branch_name = \"main\"\n",
    "path = \"tweets.parquet\"\n",
    "lakefs_s3_path = f\"s3://{repo_name}/{branch_name}/{path}\"\n",
    "df = pd.read_parquet(\n",
    "    lakefs_s3_path,\n",
    "    storage_options=storage_options,\n",
    "    engine='pyarrow',\n",
    ")\n",
    "df.sort_values(by=['postTimeRaw'], ascending=True, inplace=True)\n",
    "df.drop_duplicates(subset=\"tweetText\")\n",
    "df['index'] = df.index + 1\n",
    "df['postTimeRaw'] = df['postTimeRaw'].dt.strftime('%Y-%m-%d')\n",
    "print(df.shape)\n",
    "display(df.tail(5))\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'postTimeRaw': '2018-10-01', 'tweetText': \"Scholars at TU's Institute for Trauma, Adversity and Injustice including law professor Mimi Marton, offer clarity about the nature of interpersonal violence to combat misinformation. #TULaw #utulsa http://bit.ly/2Nfp9mP\", 'index': 1}, {'postTimeRaw': '2018-10-02', 'tweetText': 'Would you like to know more about attending the #1 best value private law school in the U.S.? The admissions team at TU Law is holding an informational session on Wednesday, Oct. 3, from noon-1 p.m. at 3120 E. 4th Place in Tulsa, Room 2442. #utulsa #TULaw http://bit.ly/2NPt6Uo', 'index': 8}, {'postTimeRaw': '2018-10-03', 'tweetText': 'The latest issue of the Energy Law Journal is available. Through a partnership between TU Law students & the Energy Bar Association, this peer-reviewed legal publication provides relevant commentary on regulatory and energy issues. #TULaw #utulsa http://bit.ly/2OB6cjy', 'index': 13}, {'postTimeRaw': '2018-10-03', 'tweetText': 'The latest issue of the Energy Law Journal is available. Through a partnership between TU Law students & the Energy Bar Association, this peer-reviewed legal publication provides relevant commentary on regulatory and energy issues. #TULaw #utulsa http://bit.ly/2xUXAuu', 'index': 12}, {'postTimeRaw': '2018-10-04', 'tweetText': 'Are you interested in earning your Master of Jurisprudence in Energy or Indian Law online? Join us for a free webinar October 16 from 6-7 p.m. to learn how working professionals can participate in this innovative online program. #TULaw #utulsa http://bit.ly/2O22Oyw', 'index': 19}]\n"
     ]
    }
   ],
   "source": [
    "df_dict:dict = df[['postTimeRaw', 'tweetText', 'index']].to_dict(orient='records')\n",
    "print(df_dict[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY  = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY \"] = GEMINI_API_KEY \n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "# Initialize client\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "instruction = \"\"\"\n",
    "คุณทำหน้าที่ในฝ่ายประชาสัมพันธ์ของมหาวิทยาลัย เป้าหมายของคุณคือการรวบรวมและจัดกลุ่ม \n",
    "\"คำถามที่พบบ่อย\" (FAQ) หรือ \"ปัญหาที่พบบ่อย\" (Issue) จากโซเชียลมีเดีย \n",
    "เพื่อใช้ในการตัดสินใจว่าควรสื่อสารผ่าน PR หรือรายงานต่อหน่วยงานที่เกี่ยวข้อง\n",
    "โดยปัญหา / คำถามที่นำมาจัดกลุ่มจะต้องเกี่ยวข้องและแก้ไขได้ในระดับมหาวิทยาลัย\n",
    "คุณจะได้รับข้อความจากโซเชียลมีเดียที่เกี่ยวข้องกับมหาวิทยาลัย\n",
    "\n",
    "คำแนะนำในการจัดกลุ่ม:\n",
    "1. **ระบุประเภท**: แยกระหว่าง \"คำถาม\" (faq) และ \"ปัญหา\" (issue)\n",
    "   - คำถาม (faq): ข้อความที่ผู้ใช้ต้องการข้อมูลหรือคำแนะนำ / ประโยคคำถาม **ต้องเป็นคำถามที่ทางมหาวิทยาลัยสามารถตอบได้** และ เกี่ยวข้องกับทางมหาวิทยาลัยโดยตรง\n",
    "   - ปัญหา (issue): ข้อความที่ผู้ใช้ระบุถึงความไม่พึงพอใจหรือสิ่งที่ต้องแก้ไข **ต้องเป็นปัญหาที่ทางมหาวิทยาลัยสามารถแก้ไขได้** และ เกี่ยวข้องกับทางมหาวิทยาลัยโดยตรง\n",
    "\n",
    "2. **ระบุหมวดหมู่ (topic)**:\n",
    "   - ใช้หมวดหมู่ที่มีอยู่แล้วหากข้อความใหม่เข้ากับหมวดหมู่เดิม\n",
    "   - สร้างหมวดหมู่ใหม่เมื่อไม่มีหมวดหมู่เดิมที่เหมาะสม\n",
    "   - ตั้งชื่อหมวดหมู่ให้กระชับ เข้าใจง่าย และมีความเฉพาะเจาะจงในระดับที่เหมาะสม\n",
    "   - ข้อความหนึ่งสามารถอยู่ได้หลายหมวดหมู่หากมีความเกี่ยวข้อง\n",
    "   - ต้องเกี่ยวข้องกับทางมหาวิทยาลัยโดยตรง และสามารถแก้ไขได้ในระดับมหาวิทยาลัย\n",
    "\n",
    "3. **ระบุหมวดหมู่ย่อย (subtopic)**:\n",
    "   - ระบุหมวดหมู่ย่อยที่มีความเฉพาะเจาะจงมากขึ้น\n",
    "   - สามารถมีได้หลายหมวดหมู่ย่อยต่อหนึ่งข้อความ\n",
    "   - หมวดหมู่ย่อยควรให้รายละเอียดเพิ่มเติมที่เป็นประโยชน์เกี่ยวกับคำถามหรือปัญหานั้น ๆ\n",
    "   - ต้องเกี่ยวข้องกับทางมหาวิทยาลัยโดยตรง และสามารถแก้ไขได้ในระดับมหาวิทยาลัย\n",
    "\n",
    "4. **พิจารณาเฉพาะข้อความที่เกี่ยวข้อง**:\n",
    "   - พิจารณาเฉพาะข้อความที่เป็นคำถามหรือปัญหาเท่านั้น\n",
    "   - ข้อความทั่วไป ข้อความสนทนา หรือข้อความที่ไม่มีเนื้อหาเป็นคำถามหรือปัญหา ไม่ต้องนำมาจัดกลุ่ม\n",
    "\n",
    "ตอบกลับมาในรูปแบบ JSON โดยมีโครงสร้างดังนี้:\n",
    "{\n",
    "    \"issue\": [\n",
    "        {\"index\": 1, \"text\": \"ข้อความ\", \"topic\": [\"หมวดหมู่1\", \"หมวดหมู่2\"], \"subtopic\": [\"หมวดย่อย1\", \"หมวดย่อย2\"]},\n",
    "        {\"index\": 2, \"text\": \"ข้อความ\", \"topic\": [\"หมวดหมู่1\"], \"subtopic\": [\"หมวดย่อย1\"]}\n",
    "    ],\n",
    "    \"faq\": [\n",
    "        {\"index\": 1, \"text\": \"ข้อความ\", \"topic\": [\"หมวดหมู่1\"], \"subtopic\": [\"หมวดย่อย1\", \"หมวดย่อย2\"]},\n",
    "        {\"index\": 2, \"text\": \"ข้อความ\", \"topic\": [\"หมวดหมู่1\", \"หมวดหมู่2\"], \"subtopic\": [\"หมวดย่อย1\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "หากไม่มีคำถามหรือปัญหาที่เกี่ยวข้อง ให้ส่งคืนค่าเป็น empty array ในหมวดนั้น\n",
    "\"\"\"\n",
    "\n",
    "# Template for use with input data\n",
    "prompt_template = \"\"\"\n",
    "# topic ของคำถามที่พบบ่อยในอดีต (FAQ) - ใช้เป็นตัวอย่างในการจัดกลุ่ม:\n",
    "{faq_topic}\n",
    "\n",
    "# sub topic ของคำถามที่พบบ่อยในอดีต (FAQ) - ใช้เป็นตัวอย่างในการจัดกลุ่ม:\n",
    "{faq_subtopic}\n",
    "\n",
    "# topic ของปัญหาที่พบบ่อยในอดีต (Issue) - ใช้เป็นตัวอย่างในการจัดกลุ่ม:\n",
    "{issue_topic}\n",
    "\n",
    "# sub topic ของปัญหาที่พบบ่อยในอดีต (Issue) - ใช้เป็นตัวอย่างในการจัดกลุ่ม:\n",
    "{issue_subtopic}\n",
    "\n",
    "# ตัวอย่างการจัดกลุ่ม:\n",
    "text: 'ไฟล์สมัครในเว็บมธ.อยู่ตรงไหนเหรอคะ มีใครพอจะทราบไหมคะ' \n",
    "topic: ['สอบถามเอกสาร']\n",
    "subtopic: ['เอกสารการสมัคร', 'การเข้าถึงข้อมูล']\n",
    "\n",
    "text: 'หอในเปิดปิดกี่โมง มีเคอร์ฟิวไหม แล้วถ้าเข้าหอดึกต้องทำยังไงบ้าง'\n",
    "topic: ['หอพัก']\n",
    "subtopic: ['กฎระเบียบหอพัก', 'เวลาเปิด-ปิด']\n",
    "\n",
    "text: 'ระบบลงทะเบียนล่มอีกแล้ว ทำไมเกิดปัญหาทุกเทอมเลย'\n",
    "topic: ['ระบบลงทะเบียน', 'ปัญหาเทคนิค']\n",
    "subtopic: ['ระบบล่ม', 'ความเสถียรของระบบ']\n",
    "\n",
    "# ข้อความที่ต้องการจัดกลุ่ม:\n",
    "{messages}\n",
    "\n",
    "โปรดวิเคราะห์และจัดกลุ่มข้อความตามคำแนะนำที่ให้ไว้ และส่งคืนเป็น JSON ตามรูปแบบที่กำหนด\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "faq_topic, faq_subtopic = set(), set()\n",
    "issue_topic, issue_subtopic = set(), set()\n",
    "\n",
    "def classify_messages(\n",
    "    tweets_eles: list,\n",
    "    faq_topic: str = faq_topic,\n",
    "    faq_subtopic: str = faq_subtopic,\n",
    "    issue_topic: str = issue_topic,\n",
    "    issue_subtopic: str = issue_subtopic\n",
    "    ) -> dict:\n",
    "    \"\"\"\n",
    "    Classify messages into issues and FAQs.\n",
    "    \n",
    "    Args:\n",
    "        messages (list): List of messages to classify.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Classified messages in JSON format.\n",
    "    \"\"\"\n",
    "    prompt_formatted = prompt_template.format(\n",
    "        faq_topic = faq_topic,\n",
    "        faq_subtopic = faq_subtopic,\n",
    "        issue_topic = issue_topic,\n",
    "        issue_subtopic = issue_subtopic,\n",
    "        messages=\"\\n\".join([f\"{row['index']}: {row['tweetText']}\" for row in tweets_eles]),\n",
    "    )\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt_formatted,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=instruction,\n",
    "            temperature=0.2, # low temperature for more deterministic output kub\n",
    "        ),\n",
    "    )\n",
    "    response_text = response.text\n",
    "    response_json = response_text[response_text.index(\"{\"): response_text.rindex(\"}\") + 1]\n",
    "    response_json = response_json.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "    response_json = json.loads(response_json, strict=False)\n",
    "    return response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ :\n",
      "[{'index': 26, 'text': 'ตามหา #เสื้อเชียร์ธรรมศาสตร์ ไซส์ XL 1ตัวค่าาาา ต้องการอย่างมากๆๆๆๆๆๆๆ มีใครรับหิ้ว หรือจะขายต่อ เชิญติดต่อทางนี้ด่วนค่าาาาา\\nID LINE : 0941478342 \\nหรือdm มาก้ได้ ตอบเร็วมากกก\\n#เสื้อเชียร์ธรรมศาสตร์ \\n#tu83 #tucheer \\n#tu #ทีมธรรมศาสตร์\\n#มธ #นิติมธ #tulaw', 'topic': ['สินค้าและบริการ'], 'subtopic': ['เสื้อเชียร์', 'ตามหา/ซื้อขาย']}, {'index': 46, 'text': 'ข้อสอบเรียงความสอบตรงนิติมธ.61\\nการอนุญาตใช้กัญชาในประเทศไทย\\nติวสอบตรงนิติมธ.&LLB 2563 ม.4,5,6 เริ่มได้แล้วน๊า \\nสอบถาม  https://line.me/R/ti/p/%40geniuslawtu…\\n#สอบตรงนิติมธ #นิติศาสตร์ #นิติมธ #ทีมมธ #ทีมนิติมธ #geniuslawtu #tcas62 #tcas63 #tcas64 #dek63 #gatpat #tulaw #tu', 'topic': ['การสอบ', 'หลักสูตร'], 'subtopic': ['สอบตรง', 'นิติศาสตร์', 'LLB']}, {'index': 47, 'text': 'ข้อสอบจริงคณะนิติมธ.หลักประชาธิปไตย\\nติวสอบตรงนิติมธ & LLB 2563\\nม.4,5,6 ห้ามพลาด\\nLINE https://line.me/R/ti/p/%40tsb8010k… \\n#สอบตรงนิติมธ #นิติมธ #lawtu #ทีมมธ #ทีมนิติมธ #gatpat #onet #dek63 #tcas62 #tcas63 #tcas64 #tcas65 #dek64 #ติวเตอร์ #สอนพิเศษ #กฎหมาย #tulaw #lawtu', 'topic': ['การสอบ', 'หลักสูตร'], 'subtopic': ['สอบตรง', 'นิติศาสตร์', 'LLB']}, {'index': 48, 'text': 'คำในกฎหมายที่มักเขียน\\n#สอบตรงนิติศาสตร์ธรรมศาสตร์\\n#geniuslawtu #สอบตรงนิติมธ #นิติมธ #lawtu #ทีมมธ #ทีมนิติมธ #gatpat #onet #dek63 #tcas62 #tcas63 #tcas64 #tcas65 #dek64 #ติวเตอร์ #สอนพิเศษ #กฎหมาย #tulaw #lawtu #เด็กนิติ #นิติศาสตร์', 'topic': ['การสอบ', 'กฎหมาย'], 'subtopic': ['สอบตรง', 'นิติศาสตร์', 'คำศัพท์กฎหมาย']}, {'index': 50, 'text': 'รอบ 2 TCAS62 โครงการจิตอาสา\\nนิติมธ.รังสิตรับ 5 คน\\nนิติมธ.ลำปางรับ 2 คน\\n\\nน้องๆ ที่จะยื่นคณะนิติมธ. ในรอบ2 TCAS เตรียมพร้อมดังนี้\\nPortด้านจิตอาสา\\nGAT\\nเข้าค่าย + สัมภาษณ์\\n#นิติมธ #ทีมมธ #ทีมนิติมธ #tcas62 #tcas63 #tcas64 #geniuslawtu #dek63 #dek64 #tulaw', 'topic': ['TCAS', 'การรับสมัคร'], 'subtopic': ['รอบ 2', 'โครงการจิตอาสา', 'นิติศาสตร์']}]\n",
      "==================================================\n",
      "Issue :\n",
      "[]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "first_50_rows = df_dict[0:50]\n",
    "first_50_rows_response = classify_messages(first_50_rows)\n",
    "\n",
    "print(\"FAQ :\")\n",
    "print(first_50_rows_response['faq'][0:5])\n",
    "print(\"=\"* 50)\n",
    "print(\"Issue :\")\n",
    "print(first_50_rows_response['issue'][0:5])\n",
    "print(\"=\"* 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rows 0 to 50\n",
      "Processing rows 50 to 100\n",
      "Processing rows 100 to 150\n",
      "Processing rows 150 to 200\n",
      "Processing rows 200 to 250\n",
      "Processing rows 250 to 300\n",
      "Processing rows 300 to 350\n",
      "Processing rows 350 to 400\n",
      "Processing rows 400 to 450\n",
      "Processing rows 450 to 500\n",
      "Processing rows 500 to 550\n",
      "Processing rows 550 to 600\n",
      "Processing rows 600 to 650\n",
      "Processing rows 650 to 700\n",
      "Processing rows 700 to 750\n",
      "Processing rows 750 to 800\n",
      "Processing rows 800 to 850\n",
      "Processing rows 850 to 900\n",
      "Processing rows 900 to 950\n",
      "Processing rows 950 to 1000\n",
      "Processing rows 1000 to 1050\n"
     ]
    }
   ],
   "source": [
    "step = 50\n",
    "prev_stop = 0\n",
    "\n",
    "all_response = []\n",
    "\n",
    "for ind in range(step, len(df_dict) + step, step):\n",
    "    start = prev_stop\n",
    "    stop = ind\n",
    "    prev_stop = stop\n",
    "    rows = df_dict[start:stop]\n",
    "    print(f\"Processing rows {start} to {stop}\")\n",
    "    \n",
    "    response = classify_messages(rows)  # already a dict\n",
    "    all_response.append(response)\n",
    "\n",
    "    # Update the sets\n",
    "    for row in response.get('issue', []):\n",
    "        for topic in row.get('topic', []):\n",
    "            issue_topic.add(topic)\n",
    "        for subtopic in row.get('subtopic', []):\n",
    "            issue_subtopic.add(subtopic)\n",
    "    \n",
    "    for row in response.get('faq', []):\n",
    "        for topic in row.get('topic', []):\n",
    "            faq_topic.add(topic)\n",
    "        for subtopic in row.get('subtopic', []):\n",
    "            faq_subtopic.add(subtopic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "faqs = [faq for response in all_response for faq in response['faq']  ]\n",
    "faqs_df = pd.DataFrame(faqs)\n",
    "faqs_df = faqs_df.merge(\n",
    "    df[['index', 'postTimeRaw']],\n",
    "    how='left',\n",
    "    on='index'\n",
    ")\n",
    "faqs_df.to_csv(f\"{today}.csv\", index=False)\n",
    "\n",
    "issues = [issue for response in all_response for issue in response['issue']]\n",
    "issues_df = pd.DataFrame(issues)\n",
    "issues_df = issues_df.merge(\n",
    "    df[['index', 'postTimeRaw']],\n",
    "    how='left',\n",
    "    on='index'\n",
    ")\n",
    "issues_df.to_csv(f\"{today}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>subtopic</th>\n",
       "      <th>postTimeRaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>ตามหา #เสื้อเชียร์ธรรมศาสตร์ ไซส์ XL 1ตัวค่าาา...</td>\n",
       "      <td>[สินค้าและบริการ]</td>\n",
       "      <td>[เสื้อเชียร์, ตามหา/ซื้อขาย]</td>\n",
       "      <td>2019-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>ขอเชิญนักศึกษามหาวิทยาลัยธรรมศาสตร์ทุกคณะทุกชั...</td>\n",
       "      <td>[กิจกรรม, นิติศาสตร์]</td>\n",
       "      <td>[กิจกรรม, วิ่ง]</td>\n",
       "      <td>2019-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>อังคารหน้าห้ามพลาด! เสวนาเปิดมุมมองและความท้าท...</td>\n",
       "      <td>[กิจกรรม, นิติศาสตร์]</td>\n",
       "      <td>[เสวนา, กฎหมายระหว่างประเทศ]</td>\n",
       "      <td>2019-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>ข้อสอบเรียงความสอบตรงนิติมธ.61\\nการอนุญาตใช้กั...</td>\n",
       "      <td>[การสอบ, นิติศาสตร์]</td>\n",
       "      <td>[แนวข้อสอบ, ติวสอบ, สอบตรงนิติมธ, tcas62, tcas...</td>\n",
       "      <td>2019-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>ข้อสอบจริงคณะนิติมธ.หลักประชาธิปไตย\\nติวสอบตรง...</td>\n",
       "      <td>[การสอบ, นิติศาสตร์]</td>\n",
       "      <td>[แนวข้อสอบ, ติวสอบ, สอบตรงนิติมธ, tcas62, tcas...</td>\n",
       "      <td>2019-03-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  \\\n",
       "0     26  ตามหา #เสื้อเชียร์ธรรมศาสตร์ ไซส์ XL 1ตัวค่าาา...   \n",
       "1     42  ขอเชิญนักศึกษามหาวิทยาลัยธรรมศาสตร์ทุกคณะทุกชั...   \n",
       "2     44  อังคารหน้าห้ามพลาด! เสวนาเปิดมุมมองและความท้าท...   \n",
       "3     46  ข้อสอบเรียงความสอบตรงนิติมธ.61\\nการอนุญาตใช้กั...   \n",
       "4     47  ข้อสอบจริงคณะนิติมธ.หลักประชาธิปไตย\\nติวสอบตรง...   \n",
       "\n",
       "                   topic                                           subtopic  \\\n",
       "0      [สินค้าและบริการ]                       [เสื้อเชียร์, ตามหา/ซื้อขาย]   \n",
       "1  [กิจกรรม, นิติศาสตร์]                                    [กิจกรรม, วิ่ง]   \n",
       "2  [กิจกรรม, นิติศาสตร์]                       [เสวนา, กฎหมายระหว่างประเทศ]   \n",
       "3   [การสอบ, นิติศาสตร์]  [แนวข้อสอบ, ติวสอบ, สอบตรงนิติมธ, tcas62, tcas...   \n",
       "4   [การสอบ, นิติศาสตร์]  [แนวข้อสอบ, ติวสอบ, สอบตรงนิติมธ, tcas62, tcas...   \n",
       "\n",
       "  postTimeRaw  \n",
       "0  2019-01-10  \n",
       "1  2019-02-06  \n",
       "2  2019-03-15  \n",
       "3  2019-03-28  \n",
       "4  2019-03-29  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>subtopic</th>\n",
       "      <th>postTimeRaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116</td>\n",
       "      <td>#tulaw #ม็อบ17ตุลา #whatishappeninginthailand</td>\n",
       "      <td>[การเมือง, สังคม]</td>\n",
       "      <td>[การเมือง, สังคม]</td>\n",
       "      <td>2020-10-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>182</td>\n",
       "      <td>อยากสอบถามค่ะะะ พอดีเราอยากแก้ไขไฟล์ที่ส่งไปให...</td>\n",
       "      <td>[ระบบรับสมัคร, TCAS]</td>\n",
       "      <td>[ระบบสมัคร, การแก้ไขข้อมูล]</td>\n",
       "      <td>2023-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221</td>\n",
       "      <td>#ธรรมศาสตร์ช้างเผือก มันไม่ขึ้นให้ตรวจสอบอ่ะค่...</td>\n",
       "      <td>[ระบบรับสมัคร, TCAS, การเข้าถึงข้อมูล]</td>\n",
       "      <td>[ระบบสมัคร, การเข้าถึงข้อมูล, ระบบล่ม]</td>\n",
       "      <td>2023-02-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189</td>\n",
       "      <td>ธรรมศาสตร์ช้างเผือกถ้าหาเกรดประถมกับม.ต้นไม่เจ...</td>\n",
       "      <td>[TCAS, การรับสมัคร]</td>\n",
       "      <td>[เอกสาร, เอกสารการสมัคร]</td>\n",
       "      <td>2023-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>212</td>\n",
       "      <td>ทำไมเว็บล่มอะ กลัวเขาแก้ไขรายชื่อละรี่หลุด  #ธ...</td>\n",
       "      <td>[ระบบรับสมัคร, TCAS]</td>\n",
       "      <td>[ระบบล่ม, ความเสถียรของระบบ]</td>\n",
       "      <td>2023-02-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  \\\n",
       "0    116      #tulaw #ม็อบ17ตุลา #whatishappeninginthailand   \n",
       "1    182  อยากสอบถามค่ะะะ พอดีเราอยากแก้ไขไฟล์ที่ส่งไปให...   \n",
       "2    221  #ธรรมศาสตร์ช้างเผือก มันไม่ขึ้นให้ตรวจสอบอ่ะค่...   \n",
       "3    189  ธรรมศาสตร์ช้างเผือกถ้าหาเกรดประถมกับม.ต้นไม่เจ...   \n",
       "4    212  ทำไมเว็บล่มอะ กลัวเขาแก้ไขรายชื่อละรี่หลุด  #ธ...   \n",
       "\n",
       "                                    topic  \\\n",
       "0                       [การเมือง, สังคม]   \n",
       "1                    [ระบบรับสมัคร, TCAS]   \n",
       "2  [ระบบรับสมัคร, TCAS, การเข้าถึงข้อมูล]   \n",
       "3                     [TCAS, การรับสมัคร]   \n",
       "4                    [ระบบรับสมัคร, TCAS]   \n",
       "\n",
       "                                 subtopic postTimeRaw  \n",
       "0                       [การเมือง, สังคม]  2020-10-17  \n",
       "1             [ระบบสมัคร, การแก้ไขข้อมูล]  2023-01-16  \n",
       "2  [ระบบสมัคร, การเข้าถึงข้อมูล, ระบบล่ม]  2023-02-14  \n",
       "3                [เอกสาร, เอกสารการสมัคร]  2023-01-09  \n",
       "4            [ระบบล่ม, ความเสถียรของระบบ]  2023-02-14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(faqs_df.head())\n",
    "display(issues_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
