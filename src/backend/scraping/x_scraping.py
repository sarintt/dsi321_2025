import urllib.parse
import asyncio
from playwright.async_api import async_playwright
import time
from datetime import datetime
import random
import pandas as pd
import os

# Import modern logging configuration
from config.logging.modern_log import LoggingConfig
# Import path configuration
from config.path_config import AUTH_TWITTER
# Import validation configuration
from src.backend.validation.validate import ValidationPydantic, TweetData
# Import LakeFS loader
from src.backend.load.lakefs_loader import LakeFSLoader

logger = LoggingConfig(level="DEBUG", level_console="DEBUG").get_logger()

class XScraping:
    def __init__(self):
        pass

    def encode_tag_to_url(self, tags: dict[str, list[str]]) -> dict[str, dict[str, str]]:
        encoded_tags_by_category = {}

        for category, tag_list in tags.items():
            encoded_tags = {}
            for i, tag in enumerate(tag_list):
                text_encoded = urllib.parse.quote(tag, safe="")
                target_url = f"https://x.com/search?q={text_encoded}&src=typed_query&f=live"
                logger.debug(f"Encoded tag {i+1}/{len(tag_list)} in '{category}': {tag}")
                encoded_tags[tag] = target_url
            encoded_tags_by_category[category] = encoded_tags

        logger.info(f"Encoded tags for {len(tags)} categories to URL format")
        return encoded_tags_by_category

    async def wait_for_articles_with_retry(self, page, max_retries: int =2) -> bool:
        for retry in range(max_retries):
            if await self.is_article_present(page):
                return True
            logger.warning(f"Retry {retry+1}/{max_retries} - Waiting before next try...")
            await asyncio.sleep(random.uniform(10.0, 22.0))
        return False

    async def is_article_present(self, page) -> bool:
        try:
            await page.wait_for_selector("article", timeout=5000)
            logger.debug("Found article on the page")
            return True
        except TimeoutError as t:
            logger.error(f"X Blocked us Please try again later üò¢")
            await page.screenshot(path="tmp/debug_screenshot_no_tweets.png")
            return False

    async def extract_articles(self, category: str, tag: str, count_tweets: int, articles: list, seen_pairs: set, all_tweet_entries: list) -> None:
        for i, article in enumerate(articles):
            displayName = await article.query_selector("[data-testid='User-Name']")
            if displayName:
                spans = await displayName.query_selector_all("span")
                time_tag = await displayName.query_selector("time")
                asyncio.sleep(random.uniform(5.0, 11.5))
                tweetText_tag = await article.query_selector("[data-testid='tweetText']")
                if len(spans) > 3 and time_tag and tweetText_tag:
                    if len(spans) == 4:
                        userName = await spans[2].text_content()
                    else:
                        userName = await spans[3].text_content()
                    userName = userName.strip()
                    dateTime = await time_tag.get_attribute("datetime")
                    tweetText = await tweetText_tag.text_content()
                    tweetText = tweetText.strip()

                    if userName and tweetText and dateTime:
                        try:
                            dt_naive = datetime.strptime(dateTime, "%Y-%m-%dT%H:%M:%S.%fZ")
                            now = datetime.now()
                            key = (userName, tweetText)
                            if key not in seen_pairs:
                                seen_pairs.add(key)
                                all_tweet_entries.append({
                                    "category": category,
                                    "tag": tag,
                                    "username": userName,
                                    "tweetText": tweetText,
                                    "postTimeRaw": dt_naive,
                                    "scrapeTime": now.strftime("%Y-%m-%dT%H:%M:%S")
                                })
                                count_tweets += 1
                                logger.debug(f"Scraped tweet {count_tweets} - {tag}")
                        except ValueError as e:
                            logger.error(f"Invalid datetime format: {dateTime} | Error: {e}", exc_info=True)
                        
                else:
                    logger.debug("Tweet does not have the expected structure.")
            else:
                logger.debug("No display name found for the article.")

    async def scrape_all_tweet_texts(self, category: str, tag: str, tag_url: str, max_scrolls: int = 10, view_browser: bool = True) -> list[dict]:
        logger.debug(f"Starting scraping: {tag}")
        all_tweet_entries = []
        seen_pairs = set() 
        count_tweets = 0
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=view_browser)
            context = await browser.new_context(
                storage_state=AUTH_TWITTER,
                viewport={"width": 1280, "height": 1024}
            )
            page = await context.new_page()
            await page.goto(tag_url)
            await asyncio.sleep(random.uniform(10, 20.0))

            # Check if the page has loaded tweets
            if not await self.wait_for_articles_with_retry(page):
                logger.error(f"No articles found for tag: {tag} (Initial load)")
                await browser.close()
                return all_tweet_entries

            now_height = 0
            for i in range(max_scrolls):
                if i > 0:
                    scroll_distance = random.randint(2800, 3800)
                    await page.evaluate(f"window.scrollBy(0, {scroll_distance});")
                    logger.debug(f"Scroll attempt {i+1}/{max_scrolls} - Scrolling by {scroll_distance}px")
                    await asyncio.sleep(random.uniform(10, 17))
                    # Check if the page has loaded tweets
                    if not await self.wait_for_articles_with_retry(page):
                        logger.warning(f"No articles found on scroll {i+1}")
                        break
                
                logger.debug(f"Scroll attempt {i+1}/{max_scrolls} - {tag}")
                new_height = await page.evaluate("document.body.scrollHeight")
                await asyncio.sleep(random.uniform(9, 14))
                logger.debug(f"Now height: {now_height} - New height after scroll: {new_height}")
                
                if new_height == now_height:
                    logger.debug("Reached bottom of page or no new content loaded.")
                    break
                now_height = new_height

                articles = await page.query_selector_all("article")
                if articles:
                    await self.extract_articles(category, tag, count_tweets, articles, seen_pairs, all_tweet_entries)
                else:
                    logger.debug("No articles found on the page.")
                    break
            
            await browser.close()
            logger.info(f"Finished scraping tag: {tag} | Total tweets: {len(all_tweet_entries)}")

        return all_tweet_entries

    @staticmethod
    def to_dataframe(all_tweet: list[dict]) -> pd.DataFrame:
        logger.info(f"Converting to dataframe...")
        all_tweet = pd.DataFrame(all_tweet)
        all_tweet['category'] = all_tweet['category'].astype('string')
        all_tweet['username'] = all_tweet['username'].astype('string')
        all_tweet['tweetText'] = all_tweet['tweetText'].astype('string')
        all_tweet['tag'] = all_tweet['tag'].astype('string')

        all_tweet['year'] = all_tweet['postTimeRaw'].dt.year
        all_tweet['month'] = all_tweet['postTimeRaw'].dt.month
        all_tweet['day'] = all_tweet['postTimeRaw'].dt.day

        all_tweet['postTimeRaw'] = pd.to_datetime(all_tweet['postTimeRaw'])
        all_tweet['scrapeTime'] = pd.to_datetime(all_tweet['scrapeTime'])
        logger.info("Finished converting to dataframe.")
        return all_tweet

    @staticmethod
    def load_to_lakefs(data: pd.DataFrame, lakefs_endpoint: str):
        LakeFSLoader(host=lakefs_endpoint).load(data=data, lakefs_endpoint=lakefs_endpoint)

async def main():
    tags = {
        "‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå": [
            "#‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡πâ‡∏≤‡∏á‡πÄ‡∏ú‡∏∑‡∏≠‡∏Å",
            "#TCAS",
            "#‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏á",
            "#‡∏ó‡∏µ‡∏°‡∏°‡∏ò",
            "#‡∏°‡∏ò", 
            "#dek70", 
            "#‡∏°‡∏≠‡∏ó‡πà‡∏≠",
            "#TU89",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏ô‡∏¥‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            # "#‡∏ô‡∏¥‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
            # "#LawTU",
            # "#TUlaw",
            "#‡∏ô‡∏¥‡∏ï‡∏¥‡∏°‡∏ò",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ç‡∏ä‡∏µ":[
            "#‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏°‡∏ò",
            "##‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏°‡∏ò",
            "#BBATU",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡∏£‡∏±‡∏ê‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏°‡∏ò",
            "#LLBTU",
            "#BIRTU",
            "#singhadang",
            "#‡∏™‡∏¥‡∏á‡∏´‡πå‡πÅ‡∏î‡∏á",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡πÄ‡∏™‡∏î‡∏™‡∏≤‡∏î‡∏°‡∏ò",
            # "#EconTU",
            # "#TUeconomics",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏™‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏™‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏°‡∏ò",
            # "#SocialWorkTU",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÅ‡∏•‡∏∞‡∏°‡∏≤‡∏ô‡∏∏‡∏©‡∏¢‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤":[
            "#‡∏™‡∏±‡∏á‡∏ß‡∏¥‡∏ó‡∏°‡∏ò",
            # "#AnthroTU",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏®‡∏¥‡∏•‡∏õ‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡∏™‡∏¥‡∏ô‡∏™‡∏≤‡∏î‡∏°‡∏ò",
            "#LartsTU",
            "#BASTU",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏≤‡∏£‡∏™‡∏≤‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏°‡∏ß‡∏•‡∏ä‡∏ô":[
            "#BJMTU",
            "#JCTU",
            "#‡∏ß‡∏≤‡∏£‡∏™‡∏≤‡∏£‡∏°‡∏ò",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ":[
            "#‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏°‡∏ò",
            "#‡∏ß‡∏¥‡∏î‡∏¢‡∏≤‡∏°‡∏ò",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡∏ß‡∏¥‡∏î‡∏ß‡∏∞‡∏°‡∏ò",
            # "#EngTU",
            # "#TUengineering",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ú‡∏±‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á":[
            "#APTU",
            "#‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏°‡∏ò",
            "#‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏°‡∏ò",
            "#‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡πå‡∏°‡∏ò",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏®‡∏¥‡∏•‡∏õ‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡∏•‡∏∞‡∏Ñ‡∏≠‡∏ô‡∏°‡∏ò",
            "#‡∏™‡∏¥‡∏ô‡∏Å‡∏≥‡∏°‡∏ò",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡πÅ‡∏û‡∏ó‡∏¢‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏°‡∏ò",
            # "#MedTU",
            # "#TUmedicine",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏™‡∏´‡πÄ‡∏ß‡∏ä‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡∏™‡∏´‡πÄ‡∏ß‡∏ä‡∏°‡∏ò",
            "#‡∏Å‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏°‡∏ò",
            "#‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏°‡∏ò",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏±‡∏ô‡∏ï‡πÅ‡∏û‡∏ó‡∏¢‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡∏ó‡∏±‡∏ô‡∏ï‡∏∞‡∏°‡∏ò",
            # "#DentTU",
            # "#TUDentistry",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏°‡∏ò",
            # "#NurseTU",
            # "#TUnursing",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏™‡∏∏‡∏Ç‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#fphtu",
            "#fphthammasat",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏°‡∏ò",
            # "#PharmTU",
            # "#TUpharmacy",
        ],
        "‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå":[
            "#lsedtu",
            "#lsed",
            "#‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        ],
        "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏û‡∏±‡∏í‡∏ô‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏õ‡πã‡∏ß‡∏¢ ‡∏≠‡∏∂‡πä‡∏á‡∏†‡∏≤‡∏Å‡∏£‡∏ì‡πå":[
            "#psdsTU",
            "#‡∏ß‡∏õ‡πã‡∏ß‡∏¢",
            "#‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏õ‡πã‡∏ß‡∏¢",
            "#‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏û‡∏±‡∏í‡∏ô‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        ],
        "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°":[
            "#‡∏ô‡∏ß‡∏±‡∏ï‡∏°‡∏ò",
            "#CITU",
            "#CITUSC",
            "#CITUTU",
        ],
        # "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏™‡∏´‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£":[
        #     "#‡∏™‡∏´‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        #     "#InterdisciplinaryTU",
        # ],
        # "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÇ‡∏•‡∏Å‡∏Ñ‡∏î‡∏µ‡∏®‡∏∂‡∏Å‡∏©‡∏≤":[
        #     "#‡πÇ‡∏•‡∏Å‡∏Ñ‡∏î‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        #     "#WorldStudiesTU",
        # ],
        # "‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏™‡∏¥‡∏£‡∏¥‡∏ô‡∏ò‡∏£":[
        #     "#SIIT",
        #     "#SIITThammasat",
        # ],
        # "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥ ‡∏õ‡∏£‡∏µ‡∏î‡∏µ ‡∏û‡∏ô‡∏°‡∏¢‡∏á‡∏Ñ‡πå":[
        #     "#‡∏õ‡∏£‡∏µ‡∏î‡∏µ‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥",
        #     "#PridiTU",
        #     "#TUinternational",
        # ],
        # "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÅ‡∏û‡∏ó‡∏¢‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏à‡∏∏‡∏¨‡∏≤‡∏†‡∏£‡∏ì‡πå":[
        #     "#‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        #     "#CICM",
        #     "#CICMTU",
        # ],
        # "‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå":[
        #     "#‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        #     "#HumanResourcesTU",
        # ],
        # "‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏î‡∏µ‡∏®‡∏∂‡∏Å‡∏©‡∏≤":[
        #     "#‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏î‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        #     "#ThaiStudiesTU",
        # ],
        # "‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤":[
        #     "#‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        #     "#EastAsianStudiesTU",
        # ],
        # "‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏†‡∏≤‡∏©‡∏≤":[
        #     "#‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        #     "#LanguageInstituteTU",
        # ],
        # "‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏≠‡∏≤‡∏ì‡∏≤‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏®‡∏∂‡∏Å‡∏©‡∏≤":[
        #     "#‡∏≠‡∏≤‡∏ì‡∏≤‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        #     "#AreaStudiesTU",
        # ],
    }

    x_scraping = XScraping()
    tag_urls = x_scraping.encode_tag_to_url(tags)


    semaphore = asyncio.Semaphore(3)
    all_results = []

    async def scrape_with_limit(category: str, tag: str, url: str):
        async with semaphore:
            result = await x_scraping.scrape_all_tweet_texts(category, tag, url)
            return result
        
    tasks = []
    for category, tag_url_dict in tag_urls.items():
        for tag, url in tag_url_dict.items():
            tasks.append(scrape_with_limit(category, tag, url))

    logger.info(f"Starting scraping with {len(tasks)} tasks, max 3 concurrently...")
    # tasks = [x_scraping.scrape_all_tweet_texts(tag=tag, tag_url=tag_urls[tag]) for tag in tag_urls.keys()]
    results = await asyncio.gather(*tasks)

    for result in results:
        all_results.extend(result)

    # all_tweet_entries = [entry for result in results for entry in result]
    data = x_scraping.to_dataframe(all_results)
    logger.info(f"Total tweets scraped from all tags: {len(data)}")

    validator = ValidationPydantic(TweetData)
    is_valid = validator.validate(data)
    is_valid = True
    if is_valid:
        os.makedirs('data', exist_ok=True)
        data.to_csv('data/tweet_data.csv', index=False)
        logger.info("CSV file saved.")
        x_scraping.load_to_lakefs(data=data, lakefs_endpoint="http://localhost:8001")


if __name__ == "__main__":
    asyncio.run(main())
